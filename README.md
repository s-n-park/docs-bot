# docs-bot

# Overview
This Python script is designed to answer questions using OpenAI's GPT-4o model. It uses a combination of text embeddings and a Redis cache to efficiently retrieve and rank relevant articles based on the question asked. The script then uses these articles to provide context to the GPT-4o model, which generates the answer.

## Dependencies
The script uses the following Python libraries which you can load from the requirements.txt file:

pandas
os
openai
numpy
typing
scipy
redis
pickle

`pip install -r requirements.txt`

## Setup
- Please note that the Redis server's host and port are hardcoded in the get_embeddings function in main.py. You will need modify these values to match your Redis server's configuration.
- If you do not have Redis server setup/want to run this locally, you can set the configuration of 'use_redis' to false in main.py
- You will need to have your own Open AI API key with the GPT-4o model permissioned

## Description
This application is a Flask web server that uses OpenAI's GPT-4 model to answer questions. It works as follows:

1. The server is started by running app.py. The Flask application is defined and two routes are set up: a home route (/) and a query route (/query).

2. When a GET request is made to the home route (/), the server responds with the index.html template.

3. When a POST request is made to the query route (/query), the server extracts the form data sent by the client, which includes the OpenAI API key and the question to be answered.

4. The server then calls the get_answer function from main.py with the OpenAI API key and the question as arguments.

5. The get_answer function does the following:

   - Calls get_embeddings to retrieve embeddings from a Redis cache or compute them if they are not in the cache from the loaded csv file. The loaded file is a static file of embeddings created from docs.near.org markdown files
   
   - Requests a question from the user along with a valid OpenAI API key and then converts that question using the OpenAI embeddings model

   - Calls distances_from_embeddings to compute the distances between the question's embedding and the list of embeddings.

   - Calls create_context which retrieves the top n (set to 3) related articles based on distance from embeddings generated from the question asked.

   - Uses those top n articles and calls answer_question to generate an answer to the question using the GPT-4o model using those articles.

   - The server then returns the answer generated by the get_answer function as a JSON response to the client and injects directly into index.html file


## Deployment
1. To test locally using just Flask in development, uncomment `app.run(host='0.0.0.0', port=8080)` in app.py, comment out `pass` and run `python app.py`. You'll be able to view it locally at http://localhost:8080
2. To deploy in production ensure that `app.run(host='0.0.0.0', port=8080)` is commented out, include `pass` and then run `gunicorn --workers 3 --bind 0.0.0.0:8080 app:app`

## Future Development
1. Known issue: Doc urls were created using filepath but not necessarily the same as actual urls on docs.near.org
2. Optimizing latency, slightly faster using Redis but could still be improved for faster response time.
3. Generalize the response to send use OpenAI structure to make response more generalizeable/callable in other applications.

License
This script is released under the MIT license.
